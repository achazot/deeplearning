\chapter{Techniques utilisées}

   \section{R-CNN}

    \subsection{Principes}
      
      En vue d'aborder les problèmes de détection et de localisation d'objets, nous avons misé sur deux techniques disctintes d'analyse d'images au travers de réseaux neuronaux : 
      \begin{itemize}
       \item La proposition de régions d'intérêt (Region Proposal Network, ou RPN)
       \item La classification d'objets au sein de ces régions (CNN)
      \end{itemize}
      
      Ces deux techniques se retrouvent au sein de l'algorithme R-CNN\cite{Bib_RCNN} (Region-based CNN) implémenté en Matlab.
      Comme son nom l'indique, R-CNN tire parti de la localisation de regions d'intérêt (Region of Interest ou ROI) avant de procéder à la classification. 
      Le réseau dédié à la classification opère de manière quasi-analogue à ce que nous avons décrit précédemment pour ses premières couches. Avant les couches entièrement connectées (qui calculent
      la sortie du réseau) est intercalé le RPN. 
      Ce dernier va interpréter les résultats des convolutions en vue de soumettre un selection de régions où les objets d'intérêt sont susceptibles de se trouver. 
      Celui-ci est capable dans le même temps d'estimer la présence ou non d'un objet (objectness score) et d'en donner les limites en tout point de l'image. 
      La sortie du RPN est constituée de ces régions qui seront ensuite traîtées par les couches pleinement connectées afin de donner les résultats de la classification. 
      L'ensemble de la chaîne fait appel à plusieurs couches de convolution et est entraînable. 
      La figure suivante établit le récapitulatif de ce fonctionnement.  
      
      \begin{figure}[H]
	  \centering
	  \makebox[\textwidth]{\includegraphics[width=.5\paperwidth]{c2p1s1_rcnn}}
	  \caption{ Principe d'un \gls{R-CNN} }
	  \label{fig:c2p1s1_rcnn}
      \end{figure}
     
      
    \subsection{Approche pratique}
    
      En pratique, R-CNN a ouvert le champs à des recherches connexes visant notamment à en reprendre les principes fondateurs tout en améliorant sa vitesse de traitement \cite{Bib_FastRCNN}. 
      Nous avons retenu pour ce travail Faster-RCNN\cite{Bib_FasterRCNN}, qui est à notre connaissance l'implémentation la plus récente basée sur R-CNN. 
      Nativement implémentée en Python et en Matlab ce travail s'oriente, comme ses prédecesseurs, vers l'application de CNN à la classification et à la détection d'objets.  
      
      La classification est assurée grâce à la bibliothèque open-source Caffe, développée
      par Berkley Vision and Learning Center (BVLC)\cite{Bib_CaffeHome}. Une documentation complète du framework est disponible sur le site internet 
      officiel\cite{Bib_CaffeTuto}, décrivant notamment les éléments de référence afin de concevoir, entraîner et tester des réseaux de neurones. 
      
      L'avancée majeure de Faster R-CNN concerne la chaîne de détection, qui peut se dérouler en quasi temps-réel. 
      En effet, les précédentes incarnations de l'algorithme étaient freinées par le temps nécessaire en amont de la détection, à savoir la proposition de régions d'intérêt. 
      Parmi les méthodes de propositions de régions les plus connues, on trouve celles tirées de conversion de l'image en superpixels, ou encore celles basées sur le glissement d'une fenêtre sur la surface à traiter. 
      L'approche adoptée par Fast et Faster R-CNN est différente. 
      Elle vise à séparer le problème de proposition d'objet de celui de détection de l'objet, tout en mutualisant un certain nombre de données entre ces deux facettes. 
      les cartes de caractéritiques résultant de la convolution sont mises au service de suggestions de régions délimitant un objet (region proposal) au sein de l'image. 
      Le framework employé va donc être composé de plusieurs modules aux objectifs disctincts (qui peuvent être mis en prallèle avec ceux décrits précédemment) : 
      
      \begin{itemize}
       \item Un module RPN (Region Proposal Network)
       \item Un module de classification d'objet basé sur la bibliothèque Caffe
      \end{itemize}
      
      L'objectif d'optimisation liée à la detection d'objets est atteint par le framework, 
      puisqu'on observe une réduction du goulet d'étranglement représenté par la proposition de régions d'intérêt à 0.01 seconde par image sur GPU (sur PASCAL VOC). 
      Une autre évolution introduite par Faster R-CNN et RPN consiste à alterner l'utilisation des deux premiers modules, tout en partageant les caractéritiques résultant des convolutions respectives. 
      
      Notons également que depuis sa publication en 2015, Faster R-CNN et RPN ont compté sur plusieurs premières places à des compétitions internationales, des applications dans divers domaines tels que la détection
      d'objets 3D ou encore l'intégration à des produits commerciaux grands publics tels que Pinterest. 
      
      Dans le cadre de ce travail, nous avons eu à réaliser une interface qui nous permette d'utiliser les fonctionnalités de Faster R-CNN en C++. 
      Elle vise à manipuler les données entrantes et sortantes de le framework Caffe, afin d'exploiter les réseaux de neurones adaptés au modèle de Faster R-CNN,   
      Pour ce faire, nous nous sommes basés sur son impélmentation originale en Python, ainsi que sur divers travaux issus du web \cite{Bib_FasterRCNN_encapsulation}.  
      
      \section{Suivi d'objets}
        \subsection{Principes}
        En matière de vision par ordinateur, le suivi d'objet est une discipline visant à déterminer les positions successives d'un objet au cours d'une séquence d'images (vidéo).
        Nous traiterons ici le suivi à court terme, qui consiste à considérer comme objet une partie d'une image $n$ repérée par une position, et à retrouver la position de cet objet dans l'image $n+1$.
        
        Pour ce faire, il est nécessaire d'établir un modèle mathématique de l'objet à travers une extraction de caractéristiques comme vu en 1.1.2.
        Le vecteur de caractéristiques obtenu sera ensuite comparé à des modèles potentiellement acceptables issus de l'image $n+1$, afin de déterminer le modèle plus proche, qui reflètera l'objet d'intérêt de l'image $n$ dans l'image $n+1$, et permettra donc de déterminer sa nouvelle position.
        Une fois la nouvelle position trouvée, le modèle doit être "entraîné" de manière à refléter les possibles changements d'aspect d'un objet (à savoir ici sa forme et ses couleurs) au cours de la séquence.
        Le modèle $n$ représentant l'objet à une image $n$ est donc fonction de tous les modèles ($0$ à $n$) jugés acceptables par l'algorithme de comparaison.
        Nous avons choisi comme caractéristiques la combinaison d'un histogramme de gradients orientés (HOG) et d'un histogramme pondéré avant-plan/arrière-plan (Foreground/Background histogram), deux principes aux qualités complémentaires et aux coûts en matière de puissance de calcul faible.
        
        \subsection{Histogramme de gradient orienté}
        La technique d'extraction de l'histogramme de gradient orienté a été pour la première fois proposée par Navneet Dalal, Bill Triggs \cite{Bib_HOG}.

        L'idée qui sous-tend ce type de descripteur est que la forme d'un objet dans une image peut être décrite par la distribution de l'intensité du gradient autour de cet objet, ce gradient décrivant précisément l'évolution de l'intensité des pixels dans la zone de l'objet.
        
        Pour calculer ce descripteur, on divise l'image en régions adjacentes de petite taille (cellules), et on calcule l'intensité du gradient selon $n$ directions pour les pixels de chaque cellule.
        Ces intensités d'orientations constituent un histogramme de $n$ classes, appelé descripteur, dont la combinaison forme un vecteur de descripteurs caractéristique de l'image.
        Il devient alors aisé de comparer deux vecteurs et d'en tirer un coefficient de similitude.
        
		      \begin{figure}[H]
			  \centering
			  \makebox[\textwidth]{\includegraphics[width=.5\paperwidth]{c2p2s2_hog}}
			  \caption{ Histogramme de gradient orienté }
			  \label{fig:c2p2s2_hog}
		      \end{figure}
			  
        \subsection{Histogramme colorimétrique pondéré}
        
        Cette technique d'extraction de caractéristique, présentée récemment dans l'algorithme de suivi Staple \cite{Bib_STAPLE}, consiste à pondérer un histogramme colorimétrique classique (dans un espace RVB) en donnant un poids plus important aux couleurs présentes dans l'objet de référence et en diminuant les poids des couleurs présentes dans l'arrière plan (zone entourant l'objet de référence).
        
        En pratique, elle consiste à construire un descripteur d'une image de référence constitué de l'histogramme colorimétrique de l'avant-plan, qu'on nommera $FG^{n}$ et de celui l'arrière-plan, qu'on nommera $BG^{n}$, avec $n$ le nombre de classes de l'histogramme. 
        La probabilité $p(x,y)$ d'un pixel $(x,y)$ de l'image d'appartenir à l'objet recherché se calcule en utilisant son histogramme colorimétrique, que l'on nommera $R^{n}$.
		On a donc: 
        \begin{center}
          $ p(x,y) = R^{n}(x,y) \times \dfrac{FG^{n}}{FG^{n}+BG^{n}+0.0001}$
        \end{center}

        \subsection{Approche pratique}
        Dans notre solution, le RPN fournit la position de l'objet d'intérêt au temps $n$, nous initialisons donc le modèle de l'objet à suivre d'après cette prédiction, et le faisons évoluer au fur et à mesure des images suivantes, en fonction des modèles acceptés à chaque étape du suivi.
        L'algorithme de suivi est donc libéré de toutes considérations de classification, car la nature de l'objet est défini par des caractéristiques calculées à la première itération, et qui lui sont donc uniques et non communs à des classes d'objets. Ceci permet de discriminer un objet parmis d'autres de la même classe, ce qui n'est pas faisable simplement par Faster-RCNN.
        
		Le psoeudo-code suivant résume son fonctionnement :
	
		\begin{lstlisting}
		
		// Recuperation de la position initiale
		position <- CNN_detection()
		// Initialisation du modele par calcul des caracteristiques
		modele <- { HoG(position), FgHist(position), BgHist(position) }
		
		position_n <- position
		Pour toute image dans flux_images faire : 
			// on cherche dans toute l'image un modele correspondant
			meilleur_score <- 0
			Pour toute position_possible dans image faire:
				modele_possible <- { HoG(position_possible), 
					FgHist(position_possible), BgHist(position_possible) }
				score = comparaison(modele, modele_possible)
				Si (score > meilleur_score)
					// on a trouve la nouvelle position de l'objet
					position_n <- position_possible
					meilleur_score <- score
				Fin Si
			Fin pour
			// on met a jour le modele avec les nouvelles informations
			modele.MaJ({ HoG(position_n), FgHist(position_n), 
				BgHist(position_n) })
		Fin pour
		
		\end{lstlisting}
		
		Toutefois, la robustesse de ce type d'algorithme est relative à la complexité de la scène.
		Lors d'une perte de l'objet d'intérêt, caractérisé par un score (voir chapitre suivant), le modèle est réinitialisé avec la position fournie par le réseau de neurones lors d'une re-détection.
		
      \section{SURF}

	\subsection{Principes}
	
	SURF, pour Speeded-Up Robust Features, est une technique de reconnaissance d'objets tirés de scènes différentes inventée par Bay Herbert, Tuytelaars Tinne et Van Gool Luc \cite{Bib_SURF}.
	L'action de SURF s'effectue en deux temps -la détection de points d'intérêts d'une scène et le calcul de leurs descripteurs-  que nous allons brièvement évoquer ici. 
	
	Le premier point s'appuie sur un calcul d'\gls{intimg} qui permettra de réduire les temps de calcul, et ceci de manière scalable au regard de la taille de l'image ou des filtres utilisés.
	Ici, l'image intégrale va entrer en jeu lors de la recherche de points d'intérêts à différentes échelles, les filtres de convolution utilisés étant de type ``box-filter''. 
	Au lieu de sous-échantillonner l'image d'entrée, on pourra jouer sur la taille des filtres qui seront appliqués à l'image d'origine et diminuer significativement le temps dédié à la recherche.  
	C'est une \gls{hessMat} (ou matrice Hessienne) qui permet de localiser des structures de type ``blob'' à une échelle donnée.
	Lorsque de tels déterminants sont calculés pour toutes les échelles de filtres fixées, les points d'intérêt détectés correspondent alors aux valeurs maximales ayant été trouvées. 
	
	Le deuxième point vise à extraire une description suffisamment robuste des points d'intérêt, permettant ensuite des comparaisons qui amènent le système à reconnaître un objet dans un scène à partir d'une image d'origine.
	La technique employée par SURF à cet égard consiste en trois étapes : 
	l'affectation d'une orientation aux points d'intérêt, la construction des descripteurs basés sur les réponses à des ondelettes de Haar et enfin, la comparaison basée sur un système d'indexation rapide.  
	
	L'orientation des points d'intérêt s'effectue en calculant pour chacun d'eux la réponse à des ondelettes de Haar distribuées selon les directions $x$ et $y$. Grâce à l'image intégrale, les réponses aux ondelettes ne requièrent que six opérations. 
	La réponse directionnelle trouvée est représentée au sein d'un cercle dont le rayon est proportionnel à l'échelle à laquelle le point à été trouvé. 
	L'extraction des descripteurs consiste à sommer les réponses aux ondelettes précédemment calculées. On somme notamment les réponses liées à la direction $x$ avec celles calculées en $y$, puis on effectue une moyenne sur une région donnée afin de
	minimiser le nombre de vecteurs résultants. Le vecteur de caractéristiques est donnée par : 
	\begin{center}
	    $ \overrightarrow{\mathcal{V}} = \left( \begin{array}{c}
	    \sum{dx}	\\
	    \sum{dy}	\\
	    \sum{|dx|}	\\
	    \sum{|dy|}	\\
	  \end{array} \right)$
	\end{center}

	La technique d'indexation employée reprend l'approximation de la matrice de Hesse précédemment calculée. Cette fois ci, c'est la trace de la matrice qui est utilisée, à savoir le Laplacien. Ce dernier donne des indications en termes de 
	luminosité de l'avant et de l'arrière plan, de telle sorte que la comparaison de deux points d'intérêt disposant de caractéristiques non similaires donne un résultat négatif. Autrement dit, un point d'intérêt sombre ne correspondra
	pas avec un point d'intérêt lumineux. Notons que cette étape ne demande pas de calculs supplémentaires puisqu'elle reprend les résultats obtenus lors de la recherche des points d'intérêt. Plus précisemment, 
	la signe positif ou négatif du Laplacien nous indique si nous sommmes dans une situation où l'avant plan est sombre et l'arrière plan brillant, ou si nous sommes dans la situation inverse. 
	
	Pour conclure, l'utilisation de SURF au sein de ce projet a été motivée par deux atouts majeurs : le premier étant la rapidité intrinsèque à l'algorithme avec notamment la possibilité de l'utiliser dans un contexte de temps réél, 
	le deuxième est sa robustesse aux changements d'échelle qui nous paraît appréciable lors de traitements de séquences filmées.
	
	\subsection{Approche pratique}
	
	Une implémentation de SURF est inclue dans la librairie OpenCV \cite{Bib_SURFOpenCV}. Depuis la version 3.0 de OpenCV, il est nécessaire d'importer la classe SURF depuis le dépôt opencv-contrib et particulièrement, depuis
	le module xfeatures2d. Cette partie évoque la façon dont nous l'avons employée et à quelles fins, sachant que l'utilisation de SURF a sensiblement changée dans la version 3.0 d'OpenCV.   
	
	Pour notre part, nous utiliserons SURF afin de calculer un score qui nous permettra d'apprécier le résultat du trackeur. Premièrement, nous fournissons à SURF la fenêtre contenant l'objet de référence que nous voulons suivre.
	A chaque nouvelle position détectée, nous demandons à SURF d'effectuer une détection des points d'intérêt correspondant à l'objet de référence, et ce sur la totalité de la nouvelle scène. Enfin, le score est donné par : 
	\begin{center} 
	  $ score = \dfrac{\text{Points d'intérêt dans la zone détectée}}{\text{Total des points d'intérêt}}$
	\end{center}
	
	Si le score n'atteint pas un certain seuil, nous lançons alors un re-détection qui fait cette fois appel au réseau de neurones (Faster R-CNN). Ce choix s'explique par la vitesse moindre du 
	réseau de neurones par rapport à celle du trackeur et, parallèlement, la qualité de détection supérieure de ce premier. Il nous fallait alors imaginer une technique qui permette d'utiliser Faster R-CNN dans un contexte pertinent, 
	à savoir lorsque la détection nous apparaît défaillante au travers de localisations de points d'intérêts fournis par SURF. 
	
	Le psoeudo-code suivant résume ce fonctionnement :
	
	\begin{lstlisting}
	
	// detection de l'objet d'interet grace au reseau de neurones
	object <- CNN_detection()
	// initialisation de SURF avec l'objet de reference
	surf.initialize( object )  
	
	Pour toute img dans flux_images faire : 
	  // on recupere la nouvelle position de l'objet 
	  objpos <- track( img )
	  // calcul du score d'apres le traitement de SURF
	  score <- surf.match( img, objpos )
	  
	  Si score < SCORE_THRESHOLD
	    // on lance une redetection avec le reseau de neurones
	    object <- CNN_detection()
	    // on reinitialise SURF
	    surf.initialize( object )
	  Fin si
	
	Fin pour
	
	\end{lstlisting}
	
	